# Transformer Model: English-to-German Translation  

This project implements a **Transformer model from scratch**, inspired by the **"Attention Is All You Need"** paper. The goal is to build a **PyTorch-based English-to-German translation model**, reinforcing fundamental concepts of transformer architectures.  

## ğŸ”— GitHub Repository  
[Transformer Implementation](https://github.com/buddeshya/Transformer/tree/main)  

## ğŸ“„ Resources Used  

### Research Papers & References  
- [Attention Is All You Need (Original Paper)](https://arxiv.org/pdf/1706.03762.pdf)  
- [Annotated Transformer (Harvard NLP)](https://nlp.seas.harvard.edu/annotated-transformer/)  

### Explainers & Visual Guides  
- ğŸ¥ **[3Blue1Brownâ€™s Video Explanation](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=316s)**  
- ğŸ“– **[Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)**  
- ğŸ–¥ï¸ **[Transformer Explainer (Polo Club)](https://poloclub.github.io/transformer-explainer/)**  

### ğŸ“ Free Courses  
- [Attention in Transformers (DeepLearning.AI)](https://learn.deeplearning.ai/courses/attention-in-transformers-concepts-and-code-in-pytorch/lesson/han2t/introduction?courseName=attention-in-transformers-concepts-and-code-in-pytorch)  
- [How Transformer LLMs Work (DeepLearning.AI)](https://learn.deeplearning.ai/courses/how-transformer-llms-work/lesson/nfshb/introduction)  
